{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DONVURfLhG14",
        "outputId": "8b562c8e-a92f-4989-d74e-33f9413c005d"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "from collections import defaultdict\n",
        "from nltk.stem import PorterStemmer\n",
        "import nltk\n",
        "import math\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "eo9fqBGtq9Ns",
        "outputId": "a01c8174-d292-49b9-c98f-d10a0266857c"
      },
      "outputs": [],
      "source": [
        "# data = pd.read_csv('combined_relevant_dataset.csv')\n",
        "data = pd.read_csv('test_dataset_labeled.csv')\n",
        "data['Input Data'] = data['Name'].apply(str) + \" \" + data['Genres'].apply(str) + \" \" + data['Type'].apply(str) + \" \" + data['Aired'].apply(str) + \" \" + data['Studios'].apply(str) + \" \" + data['sypnopsis'].apply(str)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vfOSQZ5hwIz_",
        "outputId": "09049d78-e9b2-407d-ace5-48ea8790f48f"
      },
      "outputs": [],
      "source": [
        "# input=data['Input Data'].tolist()\n",
        "# len(input)\n",
        "import spacy\n",
        "import string\n",
        "\n",
        "spacy_nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "punctuations = string.punctuation\n",
        "stop_words = spacy.lang.en.stop_words.STOP_WORDS\n",
        "\n",
        "#TODO add stemming\n",
        "def tokenizer(sentence):\n",
        "    sentence = re.sub('\\'','',sentence)\n",
        "    sentence = re.sub('\\w*\\d\\w*','',sentence)\n",
        "    sentence = re.sub(' +',' ',sentence)\n",
        "    sentence = re.sub(r'\\n: \\'\\'.*','',sentence)\n",
        "    sentence = re.sub(r'\\n!.*','',sentence)\n",
        "    sentence = re.sub(r'^:\\'\\'.*','',sentence)\n",
        "    sentence = re.sub(r'\\n',' ',sentence)\n",
        "    sentence = re.sub(r'[^\\w\\s]',' ',sentence)\n",
        "    tokens = spacy_nlp(sentence)\n",
        "    tokens = [word.lemma_.lower().strip() if word.lemma_ != \"-PRON-\" else word.lower_ for word in tokens]\n",
        "    tokens = [word for word in tokens if word not in stop_words and word not in punctuations and len(word) > 2]\n",
        "    # ps = PorterStemmer()\n",
        "    # tokens = [ps.stem(word) for word in tokens]\n",
        "    return tokens\n",
        "k1 = 1.5\n",
        "b = 0.75\n",
        "data['input_data_tokenized'] = data['Input Data'].map(tokenizer)\n",
        "# collection = {data.loc[i, [\"MAL_ID\"]].values[0]: tokenizer(data.loc[i, [\"Input Data\"]].values[0]) for i in data.index}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [],
      "source": [
        "corpus = {data.loc[i, [\"MAL_ID\"]].values[0]: data.loc[i, [\"input_data_tokenized\"]].values[0]for i in data.index}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {},
      "outputs": [],
      "source": [
        "def bm25_init(corpus, eps=0.25):\n",
        "  corpus_size = 0\n",
        "  avgdl = 0\n",
        "  doc_freqs = {}\n",
        "  idf = {}\n",
        "  doc_len = {}\n",
        "  nd = {}  # word -> number of documents with word\n",
        "  num_doc = 0\n",
        "  for key, document in corpus.items():\n",
        "    corpus_size += 1\n",
        "    doc_len[key] = len(document)\n",
        "    num_doc += len(document)\n",
        "    frequencies = {}\n",
        "    for word in document:\n",
        "      if word not in frequencies:\n",
        "        frequencies[word] = 0\n",
        "      frequencies[word] += 1\n",
        "    doc_freqs[key] = frequencies\n",
        "    for word, freq in frequencies.items():\n",
        "      if word not in nd:\n",
        "        nd[word] = 0\n",
        "      nd[word] += 1\n",
        "  avgdl = float(num_doc) / corpus_size\n",
        "  # collect idf sum to calculate an average idf for epsilon value\n",
        "  idf_sum = 0\n",
        "  # collect words with negative idf to set them a special epsilon value.\n",
        "  # idf can be negative if word is contained in more than half of documents\n",
        "  negative_idfs = []\n",
        "  for word, freq in nd.items():\n",
        "    temp_idf = math.log(corpus_size - freq + 0.5) - math.log(freq + 0.5)\n",
        "    idf[word] = temp_idf\n",
        "    idf_sum += temp_idf\n",
        "    if temp_idf < 0:\n",
        "      negative_idfs.append(word)\n",
        "  average_idf = float(idf_sum) / len(idf)\n",
        "  eps = eps * average_idf\n",
        "  for word in negative_idfs:\n",
        "    idf[word] = eps\n",
        "  \n",
        "  return corpus_size, avgdl, doc_freqs, idf, doc_len\n",
        "\n",
        "\n",
        "def get_score(query, index, doc_freqs, doc_len, idf, avgdl, k1, b):\n",
        "  score = 0.0\n",
        "  df_dict = doc_freqs[index]\n",
        "  numerator_constant = k1 + 1\n",
        "  denominator_constant = k1 * (1 - b + b * doc_len[index] / avgdl)\n",
        "  for word in query:\n",
        "    if word in df_dict:\n",
        "      df = doc_freqs[index][word]\n",
        "      temp_idf = idf[word]\n",
        "      score += (temp_idf * df * numerator_constant) / (df + denominator_constant)\n",
        "  return score\n",
        "\n",
        "\n",
        "def get_scores(query, corpus, doc_freqs, doc_len, idf, avgdl, k1, b):\n",
        "  query = tokenizer(query)\n",
        "  scores = {index: get_score(query, index, doc_freqs, doc_len, idf, avgdl, k1, b) for index in corpus}\n",
        "  return scores\n",
        "\n",
        "\n",
        "def bm25_search(query, corpus, doc_freqs, doc_len, idf, avgdl, k1=1.5, b=0.75, num_best=5):\n",
        "  scores = get_scores(query, corpus, doc_freqs, doc_len, idf, avgdl, k1, b)\n",
        "  results = sorted(scores.items(), key=lambda item: item[1], reverse=True)[:num_best]\n",
        "  return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[(41405, 5.64351769518006), (42519, 1.8280201262445748), (1253, 1.786124022100845), (12857, 1.7461053106594162), (915, 1.6534879234276645)]\n"
          ]
        }
      ],
      "source": [
        "corpus_size, avgdl, doc_freqs, idf, doc_len = bm25_init(corpus)\n",
        "results = bm25_search(\"bakugan shounen brawlers\", corpus, doc_freqs, doc_len, idf, avgdl)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "ESrpxY8jsUZK"
      },
      "outputs": [],
      "source": [
        "# clean and uniform the text\n",
        "def preprocess_string(s):\n",
        "  s = re.sub(r\"won't\", \"will not\", s)\n",
        "  s = re.sub(r\"can\\'t\", \"can not\", s)\n",
        "  s = re.sub(r\"n\\’t\", \" not\", s)\n",
        "  s = re.sub(r\"\\’re\", \" are\", s)\n",
        "  s = re.sub(r\"\\’s\", \" is\", s)\n",
        "  s = re.sub(r\"\\’d\", \" would\", s)\n",
        "  s = re.sub(r\"\\’ll\", \" will\", s)\n",
        "  s = re.sub(r\"\\’t\", \" not\", s)\n",
        "  s = re.sub(r\"\\’ve\", \" have\", s)\n",
        "  s = re.sub(r\"\\’m\", \" am\", s)\n",
        "  s = re.sub(r'[0–9]+', '', s)\n",
        "  s=re.sub(r'[^\\w\\s]','' '', s)\n",
        "  s=stem.stem(s)\n",
        "  return s"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "2gM-ux94y31Q"
      },
      "outputs": [],
      "source": [
        "# def preprocess(docs):\n",
        "#   new_docs=[]\n",
        "#   for doc in docs:\n",
        "#     new_doc=[]\n",
        "#     tmp={}\n",
        "#     for word in doc.lower().split():\n",
        "#       if word not in stop_words:\n",
        "#         a=preprocess_string(word)\n",
        "#         a=stem.stem(a)\n",
        "#         tmp[a]=tmp.get(a,0)+1\n",
        "#         new_doc.append(a)\n",
        "#     f.append(tmp)\n",
        "#     new_docs.append(new_doc)\n",
        "#   return new_docs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "YzLWwO4i2sOJ"
      },
      "outputs": [],
      "source": [
        "f=[]\n",
        "tf={}\n",
        "idf={}\n",
        "k1=1.5\n",
        "b=0.75\n",
        "\n",
        "def init(docs):\n",
        "  D=len(docs)\n",
        "  for doc in docs:\n",
        "    tmp={}\n",
        "    for word in doc.lower().split():\n",
        "      if word not in stop_words:\n",
        "        a=preprocess_string(word)\n",
        "        a=stem.stem(a)\n",
        "        tmp[a]=tmp.get(a,0)+1\n",
        "    f.append(tmp)\n",
        "    for k in tmp.keys():\n",
        "      tf[k]=tf.get(k,0)+1\n",
        "  for k,v in tf.items():\n",
        "    idf[k]=math.log(D-v+0.5) - math.log(v+0.5)\n",
        "\n",
        "  avgd=sum(len(doc)+0.0 for doc in docs)/D\n",
        "  return D,avgd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "zbVJzxXZ7lnQ"
      },
      "outputs": [],
      "source": [
        "def sim(doc,index):\n",
        "  score =0.0\n",
        "  for word in doc:\n",
        "    if word not in f[index]:\n",
        "      continue\n",
        "    else:\n",
        "      d=len(f[index])\n",
        "      score +=(idf[word]* f[index][word]*(k1+1)/(f[index][word]+k1*(1-b+b*d/avgd)))\n",
        "  return score\n",
        "def sim_overall(doc):\n",
        "  scores=[]\n",
        "  for index in range(D):\n",
        "    score =sim(doc,index)\n",
        "    scores.append(score)\n",
        "  return scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "V4I6xQ_WzS1V"
      },
      "outputs": [],
      "source": [
        "D,avgd=init(input)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zBmcwO3OCSOC",
        "outputId": "95893197-5621-47ee-ca29-c5d5c9f5095d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(16214, 428.74935241149626)"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "D,avgd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V8U_0L_1Cgg4"
      },
      "outputs": [],
      "source": [
        "tf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ya0Txhn4CnEv"
      },
      "outputs": [],
      "source": [
        "idf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZF5i29IHC2Uh",
        "outputId": "4ad04b8b-b1c1-4dc6-acad-e535a6d6e6fa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[4263, 11929, 5969, 5528, 5529] [23.399245271890756, 20.37090799954332, 18.879770132141424, 15.514535571478126, 15.457183402492502]\n"
          ]
        }
      ],
      "source": [
        "##\n",
        "import heapq\n",
        "res=sim_overall('comedy')\n",
        "re1=map(res.index,heapq.nlargest(5,res))\n",
        "re2=heapq.nlargest(5,res)\n",
        "print(list(re1),re2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "vkKc9hP7ORV5"
      },
      "outputs": [],
      "source": [
        "# re1 is the index of the dataframe row number for top 5 highest, re2 is the score\n",
        "def BM25_search(string):\n",
        "  res=sim_overall(string)\n",
        "  re1=map(res.index,heapq.nlargest(5,res))\n",
        "  re2=heapq.nlargest(5,res)\n",
        "  return(list(re1),re2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fj2sxsnoOfnB",
        "outputId": "f9b2dfd1-24c3-4d32-a92b-c2168432a3cf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[4263, 903, 348, 15435, 521] [40.71399743914366, 26.941557483397894, 26.611868662928092, 26.199655764695105, 24.512150626047088]\n"
          ]
        }
      ],
      "source": [
        "BM25_search('school')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7gxF7zN9L5vw",
        "outputId": "2c7769b8-7a4f-4f53-ad0c-7fd15c06605a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "MAL_ID                                                     7245\n",
              "Name                  Sekai Meisaku Douwa: Wow! Maerchen Oukoku\n",
              "Score                                                      6.52\n",
              "Genres                                                  Fantasy\n",
              "sypnopsis     Based on Western tales from the usual suspects...\n",
              "Input Data    Sekai Meisaku Douwa: Wow! Maerchen Oukoku Fant...\n",
              "Name: 4263, dtype: object"
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data.loc[4263]"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": " IR_BM25.ipynb",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.10.4 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    },
    "vscode": {
      "interpreter": {
        "hash": "d88df46bc0668bf185eb5c261f7c596e5e3e4cc57268c38c9e746c1f14dfeb3e"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
