{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gensim\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_queries = [\"music movie\", \"bakugan battle brawlers\", \"tenchi shounen\", \"azuki daizu show\", \"comedy\", \"boy superpowers\", \"harem cats\", \"ninja fighting\", \"fairy tail\", \"2004 shounen show\"]\n",
    "\n",
    "data = pd.read_csv('test_dataset_labeled.csv')\n",
    "data['Input Data'] = data['Name'].apply(str) + \" \" + data['Genres'].apply(str) + \" \" + data['Type'].apply(str) + \" \" + data['Aired'].apply(str) + \" \" + data['Studios'].apply(str) + \" \" + data['sypnopsis'].apply(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSA Model\n",
    "import spacy\n",
    "import re\n",
    "import string\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "\n",
    "spacy_nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "punctuations = string.punctuation\n",
    "stop_words = spacy.lang.en.stop_words.STOP_WORDS\n",
    "\n",
    "def spacy_tokenizer(sentence):\n",
    "    sentence = re.sub('\\'','',sentence)\n",
    "    sentence = re.sub('\\w*\\d\\w*','',sentence)\n",
    "    sentence = re.sub(' +',' ',sentence)\n",
    "    sentence = re.sub(r'\\n: \\'\\'.*','',sentence)\n",
    "    sentence = re.sub(r'\\n!.*','',sentence)\n",
    "    sentence = re.sub(r'^:\\'\\'.*','',sentence)\n",
    "    sentence = re.sub(r'\\n',' ',sentence)\n",
    "    sentence = re.sub(r'[^\\w\\s]',' ',sentence)\n",
    "    tokens = spacy_nlp(sentence)\n",
    "    tokens = [word.lemma_.lower().strip() if word.lemma_ != \"-PRON-\" else word.lower_ for word in tokens]\n",
    "    tokens = [word for word in tokens if word not in stop_words and word not in punctuations and len(word) > 2]\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "data['input_data_tokenized'] = data['Input Data'].map(lambda x: spacy_tokenizer(x))\n",
    "input_data = data['input_data_tokenized']\n",
    "dictionary = gensim.corpora.Dictionary(input_data)\n",
    "stoplist = set('hello and if this can would should could tell ask stop come go')\n",
    "stop_ids = [dictionary.token2id[stopword] for stopword in stoplist if stopword in dictionary.token2id]\n",
    "dictionary.filter_tokens(stop_ids)\n",
    "corpus = [dictionary.doc2bow(desc) for desc in input_data]\n",
    "st_tfidf_model = gensim.models.TfidfModel(corpus, id2word=dictionary)\n",
    "st_lsi_model = gensim.models.LsiModel(st_tfidf_model[corpus], id2word=dictionary, num_topics=300)\n",
    "\n",
    "from operator import itemgetter\n",
    "\n",
    "st_tfidf_corpus = gensim.corpora.MmCorpus('test_set_tfidf_model_mm')\n",
    "st_lsi_corpus = gensim.corpora.MmCorpus('test_set_lsi_model_mm')\n",
    "st_index = gensim.similarities.MatrixSimilarity(st_lsi_corpus, num_features = st_lsi_corpus.num_terms)\n",
    "\n",
    "def search_anime(search_term, num_best=5):\n",
    "\n",
    "    query_bow = dictionary.doc2bow(spacy_tokenizer(search_term))\n",
    "    query_tfidf = st_tfidf_model[query_bow]\n",
    "    query_lsi = st_lsi_model[query_tfidf]\n",
    "\n",
    "    st_index.num_best = num_best\n",
    "\n",
    "    st_list = st_index[query_lsi]\n",
    "\n",
    "    st_list.sort(key=itemgetter(1), reverse=True)\n",
    "    st_names = []\n",
    "\n",
    "    for j, dentry in enumerate(st_list):\n",
    "\n",
    "        st_names.append (\n",
    "            {\n",
    "                'Relevance': round((dentry[1] * 100),2),\n",
    "                'Name': data['Name'][dentry[0]],\n",
    "                'Genres' : data['Genres'][dentry[0]],\n",
    "                'Synopsis' : data['sypnopsis'][dentry[0]],\n",
    "                'MAL_ID' : data['MAL_ID'][dentry[0]]\n",
    "            }\n",
    "\n",
    "        )\n",
    "        if j == (st_index.num_best-1):\n",
    "            break\n",
    "\n",
    "    return pd.DataFrame(st_names, columns=['Relevance','Name','Genres','Synopsis','MAL_ID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VSM Model\n",
    "import glob\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "from collections import OrderedDict\n",
    "\n",
    "def wordList_removePuncs(doc_dict):\n",
    "  stop = stopwords.words('english') + list(string.punctuation) + ['\\n']\n",
    "  wordList = []\n",
    "  for doc in doc_dict.values():\n",
    "    for word in word_tokenize(doc.lower().strip()): \n",
    "      if not word in stop:\n",
    "        wordList.append(word)\n",
    "  return wordList\n",
    "\n",
    "def termFrequencyInDoc(vocab, doc_dict):\n",
    "  tf_docs = {}\n",
    "  for doc_id in doc_dict.keys():\n",
    "    tf_docs[doc_id] = {}\n",
    "  \n",
    "  for word in vocab:\n",
    "    for doc_id,doc in doc_dict.items():\n",
    "      tf_docs[doc_id][word] = doc.count(word)\n",
    "  return tf_docs\n",
    "\n",
    "def wordDocFre(vocab, doc_dict):\n",
    "  df = {}\n",
    "  for word in vocab:\n",
    "    frq = 0\n",
    "    for doc in doc_dict.values():\n",
    "      # if word in doc.lower().split():\n",
    "      if word.lower() in word_tokenize(doc.lower().strip()):\n",
    "        frq = frq + 1\n",
    "    df[word] = frq\n",
    "  return df\n",
    "\n",
    "def inverseDocFre(vocab,doc_fre,length):\n",
    "  idf= {} \n",
    "  for word in vocab:     \n",
    "    idf[word] = np.log2((length+1) / doc_fre[word])\n",
    "  return idf\n",
    "\n",
    "def tfidf(vocab,tf,idf_scr,doc_dict):\n",
    "  tf_idf_scr = {}\n",
    "  for doc_id in doc_dict.keys():\n",
    "    tf_idf_scr[doc_id] = {}\n",
    "  for word in vocab:\n",
    "    for doc_id,doc in doc_dict.items():\n",
    "      tf_idf_scr[doc_id][word] = tf[doc_id][word] * idf_scr[word]\n",
    "  return tf_idf_scr\n",
    "\n",
    "def vectorSpaceModel(query, doc_dict,tfidf_scr, num_best=5):\n",
    "  query_vocab = []\n",
    "  for word in query.split():\n",
    "    if word not in query_vocab:\n",
    "      query_vocab.append(word)\n",
    "\n",
    "  query_wc = {}\n",
    "  for word in query_vocab:\n",
    "    query_wc[word] = query.lower().split().count(word)\n",
    "  \n",
    "  relevance_scores = {}\n",
    "  for doc_id in doc_dict.keys():\n",
    "    score = 0\n",
    "    for word in query_vocab:\n",
    "      score += query_wc[word] * tfidf_scr[doc_id].get(word, 0)\n",
    "    relevance_scores[doc_id] = score\n",
    "  sorted_value = OrderedDict(sorted(relevance_scores.items(), key=lambda x: x[1], reverse = True))\n",
    "  top_results = {k: sorted_value[k] for k in list(sorted_value)[:num_best]}\n",
    "  return top_results\n",
    "\n",
    "\n",
    "docs = {}\n",
    "for i in data.index:\n",
    "  docs[i] = str(data.loc[i, 'Input Data'])\n",
    "\n",
    "M = len(docs)                                 #number of files in dataset\n",
    "w_List = wordList_removePuncs(docs)           #returns a list of tokenized words\n",
    "vocab = list(set(w_List))                     #returns a list of unique words\n",
    "tf_dict = termFrequencyInDoc(vocab, docs)     #returns term frequency\n",
    "df_dict = wordDocFre(vocab, docs)             #returns document frequencies\n",
    "idf_dict = inverseDocFre(vocab,df_dict,M)     #returns idf scores\n",
    "tf_idf = tfidf(vocab,tf_dict,idf_dict,docs)   #returns tf-idf socres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAP functions\n",
    "\n",
    "def precision_at_k(r, k):\n",
    "  assert k >= 1\n",
    "  r = np.asarray(r)[:k] != 0\n",
    "  return np.mean(r)\n",
    "\n",
    "def avg_precision(r):\n",
    "  sum_p = 0\n",
    "  total_relevant = 0\n",
    "  \n",
    "  for k in range(len(r)):\n",
    "    if r[k] != 0:\n",
    "      total_relevant += 1\n",
    "      sum_p += precision_at_k(r,k+1)\n",
    "  \n",
    "  try:\n",
    "    avg_p = sum_p/total_relevant\n",
    "  except:\n",
    "    avg_p = 0\n",
    "  return avg_p\n",
    "\n",
    "def mean_avg_precision(rs):\n",
    "  sum_ave_p = 0\n",
    "  for r in rs:\n",
    "    sum_ave_p += avg_precision(r)\n",
    "  m_avg_p = sum_ave_p/len(rs)\n",
    "  return m_avg_p  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean nDCG functions\n",
    "\n",
    "def dcg_at_k(r, k):\n",
    "  dcg = r[0] # first term\n",
    "  for i in range(k-1):\n",
    "    dcg += r[i+1] / math.log2(i+2)\n",
    "    \n",
    "  return dcg\n",
    "\n",
    "def ndcg_at_k(r, k):\n",
    "  dcg = dcg_at_k(r,k)\n",
    "  \n",
    "  r = sorted(r, reverse=True)\n",
    "  idcg = dcg_at_k(r,k)\n",
    "  return dcg/idcg if idcg!=0 else 0\n",
    "\n",
    "def mean_ndcg(rs):\n",
    "  sum_ndcg = 0\n",
    "  for r in rs:\n",
    "    k = len(r)\n",
    "    sum_ndcg += ndcg_at_k(r,k)\n",
    "  m_ndcg = sum_ndcg/len(rs)\n",
    "  return m_ndcg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MRR functions\n",
    "\n",
    "def reciprocal_rank(r):\n",
    "  for i in range(len(r)):\n",
    "    if r[i] != 0 :\n",
    "      return 1/(i+1)\n",
    "  return 0\n",
    "\n",
    "def mean_reciprocal_rank(rs):\n",
    "  sum_rr = 0\n",
    "  for r in rs:\n",
    "    sum_rr += reciprocal_rank(r)\n",
    "  m_rr = sum_rr/len(rs)\n",
    "  return m_rr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "___VSM Model___\n",
      "MAP: 0.5533333333333333\n",
      "mean nDCG: 0.5592314863560516\n",
      "MRR: 0.6\n"
     ]
    }
   ],
   "source": [
    "rs_vsm = []\n",
    "for query in test_queries:\n",
    "  result = vectorSpaceModel(query,docs,tf_idf)\n",
    "  r = []\n",
    "  \n",
    "  for index in result:\n",
    "    r.append(data.loc[index, [query]].values[0])\n",
    "  rs_vsm.append(r)\n",
    "\n",
    "print(\"___VSM Model___\")\n",
    "print(f\"MAP: {mean_avg_precision(rs_vsm)}\")\n",
    "print(f\"mean nDCG: {mean_ndcg(rs_vsm)}\")\n",
    "print(f\"MRR: {mean_reciprocal_rank(rs_vsm)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "___LSA Model___\n",
      "MAP: 0.35444444444444445\n",
      "mean nDCG: 0.5186668036159905\n",
      "MRR: 0.3166666666666667\n"
     ]
    }
   ],
   "source": [
    "# evaluating LSA\n",
    "rs_lsa = []\n",
    "for query in test_queries:\n",
    "  result = search_anime(query, num_best=5)\n",
    "  result = result[\"MAL_ID\"].to_numpy()\n",
    "  r = []\n",
    "  for mal_id in result:\n",
    "    r.append(data[data[\"MAL_ID\"]==mal_id][query].values[0])\n",
    "  rs_lsa.append(r)\n",
    "\n",
    "print(\"___LSA Model___\")\n",
    "print(f\"MAP: {mean_avg_precision(rs_lsa)}\")\n",
    "print(f\"mean nDCG: {mean_ndcg(rs_lsa)}\")\n",
    "print(f\"MRR: {mean_reciprocal_rank(rs_lsa)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d88df46bc0668bf185eb5c261f7c596e5e3e4cc57268c38c9e746c1f14dfeb3e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
